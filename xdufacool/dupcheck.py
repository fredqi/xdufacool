# dupcheck.py ---
#
# Filename: dupcheck.py
# Author: Fred Qi
# Created: 2020-09-01 13:53:31(+0800)
#
# Last-Updated: 2020-09-02 16:22:36(+0800) [by Fred Qi]
#     Update #: 228
# 

# Commentary:
#
#
# 

# Change Log:
#
#
#

import os
from os import path
from pathlib import Path
import mmap
import hashlib
import argparse
import yaml
from tqdm import tqdm


def load_list(filename):
    """Load a file list generated by the find command for duplication check.
    filename: the name of list file containing files to be checked,
              one filename per line.
              A typical command line of find is:
              find <path> -type f -name '*.iso' -size +512k -fprintf <output> %h/%f\\n
    """
    with open(filename, 'r') as infile:
        data = [ln.strip() for ln in infile.readlines()]
        files = filter(lambda x: path.isfile(x), data)
    return list(files)


def load_fdupes(filename):
    """To load a file containing the list of duplicated files generated by fdupes."""
    pass


def load_duplications(filename):
    """To load a YAML file containing a dictionary of duplicated files."""
    try:
        from yaml import CLoader as Loader
    except ImportError:
        from yaml import Loader
    with open(filename, 'r') as stream:
        duplications = yaml.load(stream, Loader=Loader)
    return duplications


def fast_hash_file(filename, n_blocks=1, block_size=1024 * 64) -> str:
    mmap_options = {'length': 0,
                    'access': mmap.ACCESS_READ}
    with open(filename, 'rb') as infile:
        with mmap.mmap(infile.fileno(), **mmap_options) as buffer:
            m = hashlib.sha256()
            if n_blocks is None or n_blocks <= 0:
                n_blocks = (buffer.size() + block_size - 1) // block_size
            while n_blocks:
                data = buffer.read(block_size)
                m.update(data)
                n_blocks -= 1
    return m.hexdigest()


def get_file_size(filename):
    return str(os.stat(filename).st_size)


def filter_dupdict(hashdict, base_dir=None):
    dict_filtered = dict(filter(lambda x: len(x[1]) > 1, hashdict.items()))
    if base_dir is not None and Path(base_dir).exists():
        keys_to_ignore = []
        for key, files in dict_filtered.items():
            dup_files = [item['filename'] for item in files]
            to_keep = any(map(lambda x: is_child_path(x, base_dir), dup_files))
            if not to_keep:
                keys_to_ignore.append(key)
        for key in keys_to_ignore:
            del dict_filtered[key]
    return dict_filtered


def dupdict_to_list(dup_files):
    filelist = []
    for value in dup_files.values():
        filelist.extend(value)
    return filelist


def simplify_dupdict(dup_files):
    dup_simplified = {}
    for key, files in dup_files.items():
        dup_simplified[key] = [item['filename'] for item in files]
    return dup_simplified


def dupdict_to_yaml(yamlfile, dup_simplified):
    try:
        from yaml import CDumper as Dumper
    except ImportError:
        from yaml import Dumper

    with open(yamlfile, 'w') as output:
        yaml.dump(dup_simplified, output,
                  Dumper=Dumper, allow_unicode=True)


def is_child_path(filepath, parent):
    p, thefile = Path(parent), Path(filepath)
    return p in thefile.parents and thefile.is_file()


def remove_dupfile(duplications, base_dir):
    for _, files in duplications.items():
        indicators = list(map(lambda x: is_child_path(x, base_dir), files))
        has_a_copy = any(indicators)
        if not has_a_copy:
            continue
        for to_keep, thefile in zip(indicators, files):
            if not to_keep:
                Path(thefile).unlink(missing_ok=True)
                print('DEL ', thefile)


class FileList(object):
    def __init__(self, filelist):
        self.block_size = 1024*1024
        self.duplications = None
        self.files = [{'filename': fn} for fn in filelist]
        self.propertyFunctions = {
            'size': get_file_size,
            'sha256-partial':
                lambda fn: fast_hash_file(fn, n_blocks=1),
            'sha256':
                lambda fn: fast_hash_file(fn, n_blocks=None),
        }

    def calc_progress(self, propertyName):
        algos = {'sha256'}
        def calc_step_size(item):
            if 'size' in item and propertyName in algos:
                item['step'] = (int(item['size']) + self.block_size - 1) // self.block_size
            else:
                item['step'] = 1
        list(map(calc_step_size, self.files))
        total_steps = sum([x['step'] for x in self.files])
        unit = ' M' if propertyName in algos else ' files'
        return tqdm(total=total_steps, unit=unit)

    def filter_files(self, base_dir):
        keys_to_ignore = []
        for key, values in self.files.items():
            dup_files = [item['filename'] for item in values]
            to_keep = any(map(lambda x: is_child_path(x, base_dir), dup_files))
            if not to_keep:
                keys_to_ignore.append(key)
        for key in keys_to_ignore:
            del self.files[key]

    def collect_property(self, propertyName):
        thefunc = self.propertyFunctions[propertyName]
        pbar = self.calc_progress(propertyName)
        for thefile in self.files:
            thefile[propertyName] = thefunc(thefile['filename'])
            pbar.update(thefile['step'])

    def find_duplication(self, propertyName, base_dir=None):
        dup_files = {}
        for thefile in self.files:
            key = thefile[propertyName]
            if key in dup_files:
                dup_files[key].append(thefile)
            else:
                dup_files[key] = [thefile]
        self.duplications = filter_dupdict(dup_files, base_dir)
        self.files = dupdict_to_list(self.duplications)


def display_duplicates(dupfiles):
    lines = []
    dup_count, dup_total = 0, 0
    for key, value in dupfiles.items():
        item = f"{key}:\n\t" + '\n\t'.join(value)
        st_info = os.stat(value[0])
        dup_count += 1
        dup_total += (len(value)-1)*st_info.st_size/1024/1024
        lines.append(item)

    print(dup_count, dup_total)
    print('\n'.join(lines))


def remove_dup_files():
    """Remove duplicated files based on the results of fdupes."""

    parser = argparse.ArgumentParser(description='Remove dup files')
    parser.add_argument('--base-dir', '-d', type=str,
                        help='The base directory for reference, files should be kept')
    parser.add_argument('--backup-dir', '-b', type=str,
                        help='The backup directory where duplicated files should be removed.')
    parser.add_argument('--dup-log', '-l', type=str,
                        help='A YAML file of duplicated files for logging.')
    parser.add_argument('filelist', type=str,
                        help='List of files created by find for duplication check.')

    args = parser.parse_args()

    files = load_list(args.filelist)
    filelist = FileList(files)
    for prop in ['size', 'sha256-partial', 'sha256']:
        filelist.collect_property(prop)
        base_dir = args.base_dir if prop == 'size' else None
        filelist.find_duplication(prop, base_dir)
    dup_simplified = simplify_dupdict(filelist.duplications)
    if args.dup_log:
        dupdict_to_yaml(args.dup_log, dup_simplified)
    if args.base_dir:
        remove_dupfile(dup_simplified, args.base_dir)
#
    # with open(args.duplist, 'r') as duplist:
    #     lines = duplist.readlines()
    #
    # # print(len(lines))
    # def check_parent_path(thepath, parent):
    #     p = Path(parent)
    #     thepath = Path(thepath)
    #     parents = thepath.parents
    #     return p in parents and thepath.is_file()
    #
    # def is_basepath(thepath):
    #     return check_parent_path(thepath, args.base_dir)
    #
    # block = []
    # for line in lines:
    #     line = line.strip()
    #     if len(line) > 0:
    #         block.append(line)
    #     else:
    #         dup_base = list(filter(is_basepath, block))
    #         file_to_remove = filter(lambda x: not is_basepath(x), block)
    #         if dup_base:
    #             for filepath in file_to_remove:
    #                 Path(filepath).unlink(missing_ok=True)
    #         block = []


if __name__ == "__main__":
    remove_dup_files()
    # filelist = load_list('files-1M.txt')
    # print(len(filelist))
    # dup_files = find_same_size_files(filelist)
    # # display_duplicates(dup_files)
    # filelist = dict_to_list(dup_files)
    # dup_files = fast_duplication_check(filelist, n_blocks=1)
    # # display_duplicates(dup_files)
    # filelist = dict_to_list(dup_files)
    # dup_files = fast_duplication_check(filelist, n_blocks=-2)
    # # display_duplicates(dup_files)
    # filelist = dict_to_list(dup_files)
    # print(len(filelist))

# 
# dupcheck.py ends here
